{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Transformer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dim():\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k = k.size(-1) # get the size of the key\n",
    "        assert q.size(-1) == d_k\n",
    "\n",
    "        # dot product between queries and keys for\n",
    "        attn = torch.bmm(q, k.transpose(Dim.seq, Dim.feature)) # (Batch, Seq, Seq)\n",
    "\n",
    "        # scale the dot products\n",
    "        attn = attn / math.sqrt(d_k)\n",
    "        # normalize the weights\n",
    "        attn = torch.exp(attn)\n",
    "        \n",
    "        # fill attention weights with 0s where padded\n",
    "        if mask is not None: attn = attn.masked_fill(mask, 0)\n",
    "        attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v) # (Batch, Seq, Feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ScaledDotProductAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand(5, 10, 20)\n",
    "k = torch.rand(5, 10, 20)\n",
    "v = torch.rand(5, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3688, 0.4383, 0.5014, 0.4978, 0.4820, 0.3852, 0.4309, 0.4215,\n",
       "          0.6410, 0.4667, 0.3422, 0.4320, 0.3634, 0.2753, 0.4307, 0.4372,\n",
       "          0.4270, 0.5259, 0.4113, 0.4472],\n",
       "         [0.5009, 0.4991, 0.6007, 0.4829, 0.4348, 0.3807, 0.5088, 0.5450,\n",
       "          0.6369, 0.5633, 0.4000, 0.4159, 0.3791, 0.3352, 0.4955, 0.5622,\n",
       "          0.4917, 0.5233, 0.4838, 0.4445],\n",
       "         [0.3862, 0.4085, 0.4401, 0.4321, 0.3374, 0.4063, 0.3365, 0.3027,\n",
       "          0.3533, 0.3219, 0.2748, 0.2514, 0.2744, 0.3729, 0.4559, 0.3248,\n",
       "          0.3482, 0.4053, 0.3264, 0.3662],\n",
       "         [0.4966, 0.5104, 0.5825, 0.4997, 0.4202, 0.4039, 0.5239, 0.5089,\n",
       "          0.6435, 0.5207, 0.4235, 0.4199, 0.3800, 0.3251, 0.5095, 0.5436,\n",
       "          0.5076, 0.5465, 0.4710, 0.4859],\n",
       "         [0.4617, 0.4794, 0.5759, 0.5622, 0.4839, 0.4299, 0.4735, 0.4497,\n",
       "          0.6554, 0.5440, 0.3630, 0.4411, 0.4273, 0.3935, 0.5188, 0.4687,\n",
       "          0.4655, 0.5489, 0.4153, 0.4540],\n",
       "         [0.5680, 0.5485, 0.6615, 0.5762, 0.5188, 0.4713, 0.5498, 0.5265,\n",
       "          0.6607, 0.6009, 0.4224, 0.4619, 0.4412, 0.4178, 0.5829, 0.5550,\n",
       "          0.5460, 0.5823, 0.5101, 0.5391],\n",
       "         [0.5394, 0.4603, 0.5899, 0.4779, 0.4311, 0.3525, 0.4732, 0.5042,\n",
       "          0.5306, 0.5731, 0.3650, 0.4159, 0.3937, 0.3414, 0.5071, 0.5383,\n",
       "          0.4840, 0.4431, 0.4283, 0.4566],\n",
       "         [0.5805, 0.5555, 0.6698, 0.5718, 0.5034, 0.4619, 0.5691, 0.5210,\n",
       "          0.6721, 0.5819, 0.4380, 0.4480, 0.4416, 0.4042, 0.5706, 0.5507,\n",
       "          0.5640, 0.5786, 0.5187, 0.5520],\n",
       "         [0.5820, 0.5489, 0.6674, 0.5777, 0.5228, 0.4725, 0.5618, 0.5071,\n",
       "          0.6585, 0.5841, 0.4138, 0.4473, 0.4480, 0.4264, 0.5817, 0.5440,\n",
       "          0.5518, 0.5800, 0.5158, 0.5502],\n",
       "         [0.4889, 0.4241, 0.4873, 0.4695, 0.3602, 0.4232, 0.4592, 0.3153,\n",
       "          0.4587, 0.4523, 0.3356, 0.3522, 0.3741, 0.4025, 0.5377, 0.3908,\n",
       "          0.4392, 0.4776, 0.3381, 0.4811]],\n",
       "\n",
       "        [[0.2901, 0.4799, 0.4685, 0.6077, 0.5413, 0.2685, 0.4251, 0.5126,\n",
       "          0.7574, 0.4816, 0.4811, 0.5098, 0.3840, 0.4991, 0.4785, 0.3018,\n",
       "          0.4394, 0.5226, 0.6925, 0.5193],\n",
       "         [0.2359, 0.3892, 0.4653, 0.6049, 0.5623, 0.3688, 0.3632, 0.4304,\n",
       "          0.7131, 0.4500, 0.4788, 0.3944, 0.2706, 0.4991, 0.5174, 0.3737,\n",
       "          0.4431, 0.4453, 0.6728, 0.5227],\n",
       "         [0.2989, 0.5481, 0.5884, 0.6991, 0.6165, 0.3859, 0.5220, 0.5549,\n",
       "          0.8408, 0.5135, 0.5861, 0.5976, 0.3884, 0.6326, 0.6025, 0.4339,\n",
       "          0.4981, 0.5593, 0.7578, 0.5919],\n",
       "         [0.3116, 0.5624, 0.5768, 0.7048, 0.6044, 0.3816, 0.5234, 0.5520,\n",
       "          0.8304, 0.5255, 0.5781, 0.6037, 0.3919, 0.6543, 0.6375, 0.4274,\n",
       "          0.4900, 0.5613, 0.7638, 0.6045],\n",
       "         [0.2960, 0.5401, 0.5872, 0.6995, 0.6241, 0.3977, 0.5152, 0.5464,\n",
       "          0.8379, 0.5099, 0.5764, 0.5827, 0.3821, 0.6212, 0.6056, 0.4445,\n",
       "          0.4953, 0.5605, 0.7565, 0.5982],\n",
       "         [0.3073, 0.5413, 0.5951, 0.7095, 0.6157, 0.3936, 0.5175, 0.5617,\n",
       "          0.8484, 0.5292, 0.5911, 0.5847, 0.3971, 0.6437, 0.6048, 0.4466,\n",
       "          0.5005, 0.5574, 0.7488, 0.6026],\n",
       "         [0.3053, 0.5596, 0.5972, 0.7086, 0.6082, 0.3868, 0.5247, 0.5515,\n",
       "          0.8388, 0.5184, 0.5924, 0.6052, 0.3956, 0.6521, 0.6165, 0.4447,\n",
       "          0.4907, 0.5647, 0.7408, 0.5890],\n",
       "         [0.2943, 0.5409, 0.5654, 0.6375, 0.5157, 0.3453, 0.4978, 0.5558,\n",
       "          0.7562, 0.4969, 0.5200, 0.5625, 0.3841, 0.6398, 0.5772, 0.4281,\n",
       "          0.4264, 0.5235, 0.6598, 0.5873],\n",
       "         [0.3144, 0.5496, 0.5825, 0.6986, 0.6102, 0.3890, 0.5216, 0.5536,\n",
       "          0.8405, 0.5182, 0.5910, 0.6024, 0.3896, 0.6417, 0.6281, 0.4264,\n",
       "          0.5015, 0.5500, 0.7662, 0.5881],\n",
       "         [0.2422, 0.3987, 0.4100, 0.6229, 0.5277, 0.2495, 0.3452, 0.4132,\n",
       "          0.7101, 0.4881, 0.4538, 0.4165, 0.3351, 0.4619, 0.4358, 0.2766,\n",
       "          0.4306, 0.4970, 0.6430, 0.4785]],\n",
       "\n",
       "        [[0.5673, 0.6578, 0.6470, 0.4579, 0.4562, 0.5437, 0.7252, 0.5710,\n",
       "          0.5373, 0.7179, 0.5382, 0.3806, 0.4990, 0.6007, 0.5095, 0.6591,\n",
       "          0.6165, 0.5146, 0.5984, 0.4482],\n",
       "         [0.4563, 0.6308, 0.6335, 0.4627, 0.4060, 0.5183, 0.6510, 0.4936,\n",
       "          0.5159, 0.6986, 0.4655, 0.3657, 0.4336, 0.5531, 0.4981, 0.6141,\n",
       "          0.5829, 0.4614, 0.5427, 0.4323],\n",
       "         [0.4561, 0.5572, 0.6784, 0.4501, 0.4166, 0.5198, 0.7034, 0.5650,\n",
       "          0.4683, 0.6840, 0.4204, 0.3477, 0.4519, 0.5200, 0.4714, 0.6691,\n",
       "          0.5192, 0.4633, 0.5500, 0.4426],\n",
       "         [0.5425, 0.6554, 0.6747, 0.4857, 0.4652, 0.5612, 0.7262, 0.5689,\n",
       "          0.5206, 0.7261, 0.5257, 0.3718, 0.4997, 0.6068, 0.5128, 0.6822,\n",
       "          0.6086, 0.5231, 0.6027, 0.4650],\n",
       "         [0.5595, 0.6447, 0.6555, 0.4612, 0.4621, 0.5479, 0.7321, 0.5762,\n",
       "          0.5258, 0.7104, 0.5369, 0.3687, 0.5060, 0.6110, 0.5101, 0.6787,\n",
       "          0.6076, 0.5248, 0.6073, 0.4512],\n",
       "         [0.5709, 0.6579, 0.6552, 0.4583, 0.4759, 0.5407, 0.7230, 0.5728,\n",
       "          0.5277, 0.7118, 0.5385, 0.3734, 0.5114, 0.6146, 0.5062, 0.6676,\n",
       "          0.6209, 0.5306, 0.5929, 0.4506],\n",
       "         [0.4850, 0.5738, 0.5459, 0.4423, 0.3900, 0.5315, 0.6189, 0.4898,\n",
       "          0.4477, 0.6374, 0.4985, 0.3115, 0.4183, 0.5532, 0.4645, 0.5985,\n",
       "          0.5153, 0.4671, 0.5778, 0.4187],\n",
       "         [0.5611, 0.6591, 0.6484, 0.4872, 0.4642, 0.5414, 0.7296, 0.5699,\n",
       "          0.5371, 0.7365, 0.5271, 0.3893, 0.4900, 0.5966, 0.5151, 0.6664,\n",
       "          0.6227, 0.5032, 0.6185, 0.4663],\n",
       "         [0.4061, 0.5306, 0.5799, 0.2851, 0.3468, 0.4736, 0.5029, 0.4806,\n",
       "          0.3992, 0.5590, 0.3333, 0.2999, 0.3816, 0.4472, 0.4022, 0.5122,\n",
       "          0.4485, 0.4661, 0.3667, 0.3179],\n",
       "         [0.5441, 0.5752, 0.5848, 0.4229, 0.4183, 0.4675, 0.6771, 0.5748,\n",
       "          0.5386, 0.6393, 0.4797, 0.3653, 0.4560, 0.5438, 0.4241, 0.5739,\n",
       "          0.5682, 0.4322, 0.5933, 0.3730]],\n",
       "\n",
       "        [[0.5842, 0.4906, 0.4817, 0.5052, 0.6066, 0.6604, 0.4695, 0.5768,\n",
       "          0.5664, 0.5894, 0.6832, 0.5447, 0.5394, 0.6188, 0.6814, 0.6107,\n",
       "          0.6114, 0.3948, 0.5685, 0.5468],\n",
       "         [0.4047, 0.3803, 0.4103, 0.3704, 0.5552, 0.5239, 0.3905, 0.4680,\n",
       "          0.4702, 0.4858, 0.5580, 0.3914, 0.3729, 0.4798, 0.4849, 0.3683,\n",
       "          0.5325, 0.3340, 0.4137, 0.3969],\n",
       "         [0.5846, 0.4992, 0.4732, 0.5101, 0.6185, 0.6478, 0.4699, 0.5749,\n",
       "          0.5710, 0.6094, 0.6859, 0.5466, 0.5378, 0.6227, 0.6764, 0.6039,\n",
       "          0.6023, 0.3791, 0.5542, 0.5542],\n",
       "         [0.5862, 0.4993, 0.4818, 0.5131, 0.6028, 0.6540, 0.4728, 0.5898,\n",
       "          0.5588, 0.6109, 0.6767, 0.5376, 0.5369, 0.6236, 0.6797, 0.6158,\n",
       "          0.5960, 0.3914, 0.5841, 0.5632],\n",
       "         [0.4785, 0.4008, 0.4368, 0.4029, 0.5795, 0.5986, 0.4048, 0.4833,\n",
       "          0.5372, 0.4895, 0.6314, 0.4643, 0.4596, 0.5206, 0.5739, 0.4741,\n",
       "          0.5840, 0.3533, 0.4474, 0.4162],\n",
       "         [0.4792, 0.3814, 0.4069, 0.4530, 0.4542, 0.5872, 0.3825, 0.4513,\n",
       "          0.4179, 0.5176, 0.5773, 0.4941, 0.4280, 0.5305, 0.5575, 0.5470,\n",
       "          0.4423, 0.3200, 0.5615, 0.5195],\n",
       "         [0.5936, 0.5033, 0.4616, 0.5269, 0.6036, 0.6443, 0.4818, 0.5955,\n",
       "          0.5521, 0.6015, 0.6668, 0.5452, 0.5276, 0.6241, 0.6823, 0.6103,\n",
       "          0.5912, 0.3962, 0.5859, 0.5607],\n",
       "         [0.4500, 0.3780, 0.3625, 0.3727, 0.5563, 0.5150, 0.3862, 0.4827,\n",
       "          0.5395, 0.4532, 0.5525, 0.3727, 0.4298, 0.4795, 0.5553, 0.4128,\n",
       "          0.4838, 0.3130, 0.4372, 0.3465],\n",
       "         [0.4990, 0.3871, 0.3324, 0.4050, 0.4730, 0.4872, 0.4214, 0.4546,\n",
       "          0.4850, 0.4523, 0.4839, 0.4135, 0.3493, 0.4522, 0.5586, 0.4307,\n",
       "          0.4028, 0.2666, 0.4685, 0.3384],\n",
       "         [0.4649, 0.4399, 0.3689, 0.4395, 0.5364, 0.4905, 0.4117, 0.5515,\n",
       "          0.4667, 0.5595, 0.5223, 0.3758, 0.4181, 0.5280, 0.5467, 0.4509,\n",
       "          0.4387, 0.3230, 0.5171, 0.4763]],\n",
       "\n",
       "        [[0.4985, 0.4976, 0.3831, 0.4072, 0.4320, 0.5962, 0.5584, 0.4573,\n",
       "          0.6348, 0.4897, 0.4798, 0.5636, 0.4121, 0.4188, 0.5469, 0.5150,\n",
       "          0.4933, 0.4725, 0.3767, 0.3658],\n",
       "         [0.5588, 0.5462, 0.4910, 0.5020, 0.5163, 0.5284, 0.6581, 0.4711,\n",
       "          0.6605, 0.5626, 0.5048, 0.6243, 0.3684, 0.4405, 0.5472, 0.5729,\n",
       "          0.5086, 0.4540, 0.4715, 0.4011],\n",
       "         [0.6080, 0.6700, 0.5170, 0.5041, 0.6048, 0.6397, 0.7576, 0.4998,\n",
       "          0.7559, 0.6237, 0.5894, 0.6855, 0.4713, 0.5420, 0.6431, 0.6445,\n",
       "          0.6369, 0.5508, 0.5222, 0.4416],\n",
       "         [0.4718, 0.5286, 0.4205, 0.3906, 0.5005, 0.5070, 0.7254, 0.4236,\n",
       "          0.6264, 0.5234, 0.4868, 0.6472, 0.4651, 0.4829, 0.5071, 0.6005,\n",
       "          0.5096, 0.5081, 0.4956, 0.4129],\n",
       "         [0.5125, 0.6268, 0.4090, 0.4668, 0.6059, 0.5600, 0.6606, 0.4378,\n",
       "          0.6799, 0.5760, 0.5088, 0.5742, 0.4707, 0.5186, 0.5577, 0.5966,\n",
       "          0.6275, 0.5188, 0.4650, 0.3627],\n",
       "         [0.4900, 0.4804, 0.4240, 0.4682, 0.4023, 0.4692, 0.5041, 0.4485,\n",
       "          0.5268, 0.4672, 0.4382, 0.4798, 0.3231, 0.3673, 0.4411, 0.4422,\n",
       "          0.4285, 0.3151, 0.3457, 0.3065],\n",
       "         [0.5508, 0.6003, 0.4496, 0.4953, 0.4909, 0.5980, 0.6017, 0.5115,\n",
       "          0.6280, 0.5401, 0.5272, 0.5395, 0.4419, 0.4827, 0.5268, 0.5288,\n",
       "          0.5605, 0.4164, 0.3737, 0.3443],\n",
       "         [0.6128, 0.6802, 0.5210, 0.5068, 0.6016, 0.6533, 0.7465, 0.5092,\n",
       "          0.7510, 0.6212, 0.5922, 0.6720, 0.4684, 0.5457, 0.6452, 0.6359,\n",
       "          0.6494, 0.5397, 0.5079, 0.4270],\n",
       "         [0.4634, 0.5747, 0.4399, 0.3410, 0.4763, 0.5742, 0.5663, 0.4215,\n",
       "          0.6439, 0.5059, 0.4925, 0.5719, 0.3624, 0.4235, 0.5855, 0.4859,\n",
       "          0.5644, 0.4967, 0.4034, 0.3434],\n",
       "         [0.5802, 0.5481, 0.4487, 0.5102, 0.5054, 0.6048, 0.6540, 0.5077,\n",
       "          0.6835, 0.5441, 0.5061, 0.6239, 0.4225, 0.4683, 0.5784, 0.5924,\n",
       "          0.5481, 0.4759, 0.4705, 0.3818]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model, d_feature, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # We will assume the queries, keys, and values all have the same feature size\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries) # (Batch, Seq, Feature)\n",
    "        K = self.key_tfm(keys) # (Batch, Seq, Feature)\n",
    "        V = self.value_tfm(values) # (Batch, Seq, Feature)\n",
    "        # compute multiple attention weighted sums\n",
    "        x = self.attn(Q, K, V)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9722e-01, -4.3217e-01, -9.5496e-02,  1.0858e-01,  7.4761e-02,\n",
       "          -4.2638e-02, -5.9460e-02, -2.2475e-01,  3.8707e-01, -4.2695e-01,\n",
       "           1.3618e-01,  4.7474e-01,  1.8776e-01, -6.5161e-01, -6.7375e-01,\n",
       "           6.6575e-01,  1.5730e-01, -3.5106e-02,  4.1626e-01,  2.7541e-02],\n",
       "         [ 1.9474e-01, -4.3145e-01, -9.6251e-02,  1.0705e-01,  7.4421e-02,\n",
       "          -4.2501e-02, -6.2102e-02, -2.2431e-01,  3.8733e-01, -4.2657e-01,\n",
       "           1.3499e-01,  4.7276e-01,  1.8463e-01, -6.5237e-01, -6.7276e-01,\n",
       "           6.6625e-01,  1.5797e-01, -3.3872e-02,  4.1627e-01,  2.7826e-02],\n",
       "         [ 1.7152e-01, -4.0612e-01, -9.1967e-02,  1.0535e-01,  8.4465e-02,\n",
       "          -5.3387e-02, -7.9679e-02, -1.7022e-01,  3.6955e-01, -3.7688e-01,\n",
       "           1.2617e-01,  4.6859e-01,  1.9201e-01, -5.9551e-01, -6.4006e-01,\n",
       "           5.8396e-01,  1.3871e-01, -2.6731e-02,  3.2185e-01,  2.5310e-02],\n",
       "         [ 1.4698e-01, -3.9507e-01, -1.0065e-01,  9.1711e-02,  6.3303e-02,\n",
       "          -4.1928e-02, -9.7377e-02, -2.0138e-01,  3.4731e-01, -3.7956e-01,\n",
       "           1.2201e-01,  4.3707e-01,  1.5934e-01, -6.1838e-01, -6.1082e-01,\n",
       "           6.1645e-01,  1.7013e-01, -1.3123e-02,  3.6666e-01,  3.9213e-02],\n",
       "         [ 1.9326e-01, -4.3438e-01, -9.3702e-02,  1.0826e-01,  7.4500e-02,\n",
       "          -3.9930e-02, -5.8766e-02, -2.2439e-01,  3.9176e-01, -4.2742e-01,\n",
       "           1.3230e-01,  4.7295e-01,  1.8323e-01, -6.5398e-01, -6.7150e-01,\n",
       "           6.6968e-01,  1.5818e-01, -3.7968e-02,  4.1212e-01,  2.6498e-02],\n",
       "         [ 1.5822e-01, -3.4961e-01, -7.6393e-02,  9.8700e-02,  4.0424e-02,\n",
       "          -8.6840e-03, -3.0274e-02, -2.3083e-01,  3.0272e-01, -3.6643e-01,\n",
       "           1.0492e-01,  4.0347e-01,  1.7491e-01, -5.3552e-01, -5.4370e-01,\n",
       "           5.6139e-01,  1.2398e-01, -2.0275e-02,  3.6409e-01,  3.3320e-02],\n",
       "         [ 1.6523e-01, -3.8637e-01, -8.5707e-02,  8.1299e-02,  5.2896e-02,\n",
       "          -3.4617e-02, -6.0883e-02, -1.9174e-01,  3.6982e-01, -3.9349e-01,\n",
       "           1.2988e-01,  4.2574e-01,  1.3270e-01, -5.9617e-01, -6.0337e-01,\n",
       "           6.1251e-01,  1.2224e-01, -3.7426e-02,  3.6088e-01,  2.3988e-02],\n",
       "         [ 1.4796e-01, -3.9449e-01, -1.0271e-01,  9.0775e-02,  6.5009e-02,\n",
       "          -4.4956e-02, -1.0016e-01, -2.0106e-01,  3.4393e-01, -3.7805e-01,\n",
       "           1.2291e-01,  4.3633e-01,  1.6061e-01, -6.1706e-01, -6.1181e-01,\n",
       "           6.1392e-01,  1.7033e-01, -1.0472e-02,  3.6842e-01,  3.9921e-02],\n",
       "         [ 1.9759e-01, -4.3241e-01, -9.5245e-02,  1.0861e-01,  7.5157e-02,\n",
       "          -4.3232e-02, -5.9323e-02, -2.2460e-01,  3.8755e-01, -4.2713e-01,\n",
       "           1.3667e-01,  4.7471e-01,  1.8642e-01, -6.5162e-01, -6.7365e-01,\n",
       "           6.6601e-01,  1.5650e-01, -3.5351e-02,  4.1588e-01,  2.7214e-02],\n",
       "         [ 2.0387e-01, -3.8222e-01, -9.1450e-02,  1.0856e-01,  4.9979e-02,\n",
       "          -4.3070e-02, -4.4418e-02, -2.1850e-01,  3.3151e-01, -3.9970e-01,\n",
       "           1.6162e-01,  4.6475e-01,  2.0360e-01, -5.9120e-01, -6.2552e-01,\n",
       "           5.9904e-01,  1.4118e-01, -2.5616e-02,  4.0451e-01,  3.7522e-02]],\n",
       "\n",
       "        [[ 2.3825e-01, -3.9368e-01, -1.3332e-01,  1.5256e-01,  6.5915e-02,\n",
       "          -3.6905e-02, -1.5985e-01, -2.1921e-01,  4.0505e-01, -2.9530e-01,\n",
       "           1.6095e-01,  4.3650e-01,  2.5044e-01, -5.8272e-01, -6.6809e-01,\n",
       "           5.3050e-01,  1.8580e-01,  4.6180e-02,  3.6940e-01,  1.5564e-01],\n",
       "         [ 2.4634e-01, -3.9834e-01, -1.0183e-01,  1.8174e-01,  6.4062e-02,\n",
       "           8.3365e-03, -1.1124e-01, -2.1004e-01,  3.5936e-01, -2.5021e-01,\n",
       "           1.0168e-01,  4.1195e-01,  2.3798e-01, -5.4232e-01, -5.8801e-01,\n",
       "           4.7482e-01,  1.2824e-01,  1.8044e-02,  2.8835e-01,  1.1732e-01],\n",
       "         [ 3.0591e-01, -4.7669e-01, -1.7369e-01,  1.6921e-01,  7.3936e-02,\n",
       "          -4.0274e-02, -1.6832e-01, -2.4530e-01,  4.4612e-01, -3.2446e-01,\n",
       "           1.5759e-01,  4.9363e-01,  2.6946e-01, -6.6141e-01, -7.4534e-01,\n",
       "           6.0806e-01,  1.7328e-01,  5.5584e-02,  3.9495e-01,  1.5951e-01],\n",
       "         [ 2.8226e-01, -4.6478e-01, -1.3914e-01,  1.9881e-01,  7.6141e-02,\n",
       "          -1.8499e-02, -1.4180e-01, -2.2528e-01,  4.2979e-01, -2.8237e-01,\n",
       "           1.3830e-01,  4.6133e-01,  2.5615e-01, -6.1500e-01, -6.8485e-01,\n",
       "           5.4739e-01,  1.5232e-01,  3.0028e-02,  3.3683e-01,  1.4358e-01],\n",
       "         [ 2.4043e-01, -3.9138e-01, -1.3253e-01,  1.5007e-01,  6.4287e-02,\n",
       "          -3.7131e-02, -1.5845e-01, -2.1784e-01,  4.0369e-01, -2.9724e-01,\n",
       "           1.6070e-01,  4.3631e-01,  2.5017e-01, -5.8090e-01, -6.6810e-01,\n",
       "           5.2996e-01,  1.8378e-01,  4.6707e-02,  3.7005e-01,  1.5447e-01],\n",
       "         [ 2.6318e-01, -4.5457e-01, -1.5157e-01,  1.6780e-01,  9.0879e-02,\n",
       "          -2.3181e-02, -1.5318e-01, -2.5317e-01,  3.6101e-01, -2.7830e-01,\n",
       "           1.6252e-01,  4.6007e-01,  2.2060e-01, -6.3636e-01, -6.4459e-01,\n",
       "           5.7411e-01,  1.6934e-01,  2.5350e-02,  3.3672e-01,  1.2654e-01],\n",
       "         [ 3.0219e-01, -3.3868e-01, -1.5003e-01,  9.9183e-02,  1.4572e-02,\n",
       "          -6.8670e-02, -1.1258e-01, -1.6353e-01,  3.7614e-01, -2.4122e-01,\n",
       "           1.3295e-01,  3.7835e-01,  2.2785e-01, -4.3658e-01, -5.9606e-01,\n",
       "           4.2313e-01,  9.9106e-02,  5.5143e-02,  3.1889e-01,  1.3389e-01],\n",
       "         [ 2.6789e-01, -4.5875e-01, -1.5451e-01,  1.6936e-01,  9.1361e-02,\n",
       "          -2.5107e-02, -1.5555e-01, -2.5258e-01,  3.6480e-01, -2.7866e-01,\n",
       "           1.6324e-01,  4.6283e-01,  2.2247e-01, -6.3942e-01, -6.5027e-01,\n",
       "           5.7651e-01,  1.6782e-01,  2.5858e-02,  3.3717e-01,  1.2647e-01],\n",
       "         [ 1.8573e-01, -3.3537e-01, -1.0300e-01,  1.2270e-01,  5.2241e-02,\n",
       "          -1.8917e-02, -1.2632e-01, -2.1085e-01,  3.4873e-01, -2.8227e-01,\n",
       "           1.4647e-01,  4.0005e-01,  2.2832e-01, -5.2558e-01, -5.8589e-01,\n",
       "           4.8721e-01,  1.7777e-01,  3.9117e-02,  3.4209e-01,  1.4200e-01],\n",
       "         [ 2.2892e-01, -3.7956e-01, -1.3880e-01,  1.3113e-01,  6.3140e-02,\n",
       "          -2.1202e-02, -1.3702e-01, -2.2454e-01,  3.5552e-01, -2.7015e-01,\n",
       "           1.0574e-01,  3.8675e-01,  2.0565e-01, -5.4252e-01, -5.9179e-01,\n",
       "           5.1408e-01,  1.4693e-01,  6.0178e-02,  3.4140e-01,  1.3412e-01]],\n",
       "\n",
       "        [[ 2.6764e-01, -4.2504e-01, -1.0441e-01,  3.9053e-02,  1.0183e-01,\n",
       "          -1.2891e-01,  6.0053e-03, -1.9267e-01,  4.1918e-01, -3.6363e-01,\n",
       "           1.0065e-01,  4.7957e-01,  2.4635e-01, -5.0847e-01, -6.6053e-01,\n",
       "           6.1811e-01,  1.1560e-01, -4.4955e-02,  3.6916e-01,  4.2024e-02],\n",
       "         [ 2.7391e-01, -4.4988e-01, -1.0194e-01,  4.8484e-02,  1.3641e-01,\n",
       "          -1.1947e-01, -1.1348e-02, -2.0738e-01,  4.2147e-01, -3.9605e-01,\n",
       "           9.7138e-02,  5.1147e-01,  2.8020e-01, -5.5807e-01, -7.2328e-01,\n",
       "           6.6815e-01,  1.5546e-01, -5.4546e-02,  4.2161e-01,  1.3187e-02],\n",
       "         [ 2.7227e-01, -4.4983e-01, -1.0177e-01,  5.1413e-02,  1.3615e-01,\n",
       "          -1.1913e-01, -1.4946e-02, -2.0617e-01,  4.1999e-01, -3.9432e-01,\n",
       "           9.7285e-02,  5.0951e-01,  2.7936e-01, -5.5981e-01, -7.2234e-01,\n",
       "           6.6711e-01,  1.5925e-01, -5.6407e-02,  4.2237e-01,  1.1999e-02],\n",
       "         [ 2.4051e-01, -4.0269e-01, -1.2864e-01, -8.3666e-04,  7.1724e-02,\n",
       "          -1.4111e-01,  5.9536e-03, -1.2590e-01,  3.8324e-01, -3.3003e-01,\n",
       "           1.1585e-01,  4.3847e-01,  2.0858e-01, -4.4601e-01, -6.0324e-01,\n",
       "           5.5026e-01,  9.2982e-02, -3.5783e-02,  2.9778e-01,  3.4299e-02],\n",
       "         [ 2.3560e-01, -3.4165e-01, -5.5290e-02,  6.0248e-02,  1.5773e-01,\n",
       "          -8.3285e-02, -1.6416e-02, -1.7772e-01,  3.4425e-01, -2.9195e-01,\n",
       "           6.9105e-02,  4.1346e-01,  2.5850e-01, -4.1584e-01, -5.9510e-01,\n",
       "           4.7933e-01,  1.5789e-01, -7.2574e-03,  3.3188e-01,  4.8976e-02],\n",
       "         [ 2.4712e-01, -3.8751e-01, -8.7544e-02,  7.8923e-03,  1.2667e-01,\n",
       "          -7.5757e-02,  2.6821e-02, -2.1532e-01,  3.4017e-01, -3.7751e-01,\n",
       "           5.9956e-02,  4.3178e-01,  2.6110e-01, -4.8051e-01, -6.3710e-01,\n",
       "           6.0779e-01,  1.1229e-01, -4.3189e-02,  4.0322e-01, -5.5053e-05],\n",
       "         [ 2.7506e-01, -4.5224e-01, -1.0372e-01,  4.8611e-02,  1.3558e-01,\n",
       "          -1.1898e-01, -1.1253e-02, -2.0766e-01,  4.2491e-01, -3.9558e-01,\n",
       "           9.7225e-02,  5.1165e-01,  2.8233e-01, -5.5839e-01, -7.2570e-01,\n",
       "           6.6845e-01,  1.5427e-01, -5.3386e-02,  4.2055e-01,  1.6672e-02],\n",
       "         [ 1.9877e-01, -3.9610e-01, -1.3001e-01,  4.3770e-02,  7.9749e-02,\n",
       "          -1.0515e-01, -5.8259e-02, -1.1608e-01,  3.6978e-01, -3.0798e-01,\n",
       "           9.6382e-02,  4.1355e-01,  2.4555e-01, -4.6933e-01, -6.1805e-01,\n",
       "           5.3620e-01,  1.4489e-01, -4.7779e-02,  3.2184e-01,  1.7292e-02],\n",
       "         [ 2.3736e-01, -3.6914e-01, -7.2203e-02,  5.5912e-02,  1.1761e-01,\n",
       "          -1.0670e-01, -7.7923e-06, -1.8431e-01,  3.7989e-01, -3.0962e-01,\n",
       "           7.8336e-02,  4.3641e-01,  2.3548e-01, -4.4463e-01, -5.9046e-01,\n",
       "           5.3084e-01,  1.0749e-01, -2.4579e-02,  3.1918e-01,  5.1634e-02],\n",
       "         [ 2.5510e-01, -4.0832e-01, -8.0543e-02,  5.2348e-02,  1.2039e-01,\n",
       "          -9.7041e-02,  5.1407e-03, -1.8941e-01,  3.6159e-01, -3.4036e-01,\n",
       "           9.4337e-02,  4.2983e-01,  2.1323e-01, -5.0800e-01, -6.1732e-01,\n",
       "           6.0726e-01,  1.3777e-01, -7.3235e-02,  3.6422e-01,  1.6116e-03]],\n",
       "\n",
       "        [[ 1.6746e-01, -3.4555e-01, -5.4169e-02,  1.5294e-01,  9.5196e-02,\n",
       "           3.0204e-02, -8.1832e-02, -2.0785e-01,  4.2015e-01, -3.7515e-01,\n",
       "           3.8941e-02,  4.7960e-01,  2.2136e-01, -5.2170e-01, -6.7092e-01,\n",
       "           5.1852e-01,  2.0027e-01,  4.1028e-02,  3.6157e-01,  4.2391e-02],\n",
       "         [ 1.6052e-01, -3.4662e-01, -5.3427e-02,  1.5622e-01,  9.5080e-02,\n",
       "           2.7588e-02, -8.8661e-02, -2.0284e-01,  4.1850e-01, -3.7738e-01,\n",
       "           4.0163e-02,  4.8118e-01,  2.1966e-01, -5.3088e-01, -6.7214e-01,\n",
       "           5.2293e-01,  2.0623e-01,  3.7002e-02,  3.6059e-01,  4.0429e-02],\n",
       "         [ 1.7228e-01, -3.6178e-01, -3.7522e-02,  1.4603e-01,  1.0104e-01,\n",
       "           7.0660e-02, -8.4755e-02, -2.2804e-01,  4.5543e-01, -4.2684e-01,\n",
       "           3.7749e-02,  5.0422e-01,  2.2368e-01, -5.7893e-01, -7.3068e-01,\n",
       "           5.9773e-01,  2.1060e-01,  6.1381e-02,  4.1808e-01,  4.7433e-02],\n",
       "         [ 1.7075e-01, -3.1131e-01,  5.1075e-03,  1.1807e-01,  1.0033e-01,\n",
       "           7.3587e-02, -5.9200e-02, -2.3603e-01,  4.3089e-01, -3.9242e-01,\n",
       "           2.4642e-02,  4.6507e-01,  2.0719e-01, -4.9801e-01, -6.5385e-01,\n",
       "           5.4553e-01,  1.7952e-01,  6.9343e-02,  3.8300e-01,  7.2076e-02],\n",
       "         [ 1.7120e-01, -3.3509e-01, -3.8054e-02,  1.4256e-01,  1.0222e-01,\n",
       "           5.9849e-02, -5.5507e-02, -2.2154e-01,  4.1181e-01, -3.8370e-01,\n",
       "           3.3412e-02,  4.4829e-01,  1.9676e-01, -5.2295e-01, -6.5906e-01,\n",
       "           5.4869e-01,  1.6440e-01,  5.9968e-02,  3.7144e-01,  3.8196e-02],\n",
       "         [ 1.7394e-01, -3.4125e-01, -4.0184e-02,  1.5380e-01,  1.0661e-01,\n",
       "           8.2880e-02, -8.1015e-02, -2.2361e-01,  4.3429e-01, -3.6874e-01,\n",
       "           1.5930e-02,  4.6959e-01,  2.1337e-01, -5.3452e-01, -6.7026e-01,\n",
       "           5.4383e-01,  1.8777e-01,  6.6291e-02,  3.6401e-01,  4.8680e-02],\n",
       "         [ 1.3169e-01, -3.3879e-01, -3.0502e-02,  1.4295e-01,  8.1027e-02,\n",
       "           4.5109e-02, -1.0665e-01, -1.8207e-01,  3.9267e-01, -3.6689e-01,\n",
       "           9.7091e-03,  4.1411e-01,  1.8604e-01, -5.5048e-01, -6.2633e-01,\n",
       "           5.4423e-01,  1.9111e-01,  2.2743e-02,  3.5540e-01,  3.9300e-02],\n",
       "         [ 1.0589e-01, -2.0688e-01,  8.0087e-03,  9.2564e-02,  5.7248e-02,\n",
       "           6.4641e-02, -6.0575e-02, -1.8529e-01,  3.3215e-01, -3.2885e-01,\n",
       "           2.1753e-02,  3.7937e-01,  1.6525e-01, -3.8888e-01, -5.1913e-01,\n",
       "           4.2421e-01,  1.6189e-01,  6.0494e-02,  3.2129e-01,  4.6411e-02],\n",
       "         [ 1.4635e-01, -3.2536e-01, -3.9375e-02,  1.3055e-01,  1.0967e-01,\n",
       "           4.8206e-02, -6.1258e-02, -1.8911e-01,  3.4253e-01, -3.2364e-01,\n",
       "           3.4070e-02,  3.6759e-01,  1.6225e-01, -4.9618e-01, -5.6891e-01,\n",
       "           4.9745e-01,  1.4708e-01,  4.2046e-02,  3.1262e-01,  4.9907e-02],\n",
       "         [ 1.6376e-01, -2.8066e-01, -5.9983e-02,  9.7335e-02,  5.7728e-02,\n",
       "           4.3977e-02, -5.1015e-02, -1.9298e-01,  3.7138e-01, -3.3402e-01,\n",
       "           3.8394e-02,  4.0923e-01,  1.9926e-01, -4.1444e-01, -5.8857e-01,\n",
       "           4.3214e-01,  1.6340e-01,  4.8686e-02,  3.5310e-01,  3.8984e-02]],\n",
       "\n",
       "        [[ 1.8994e-01, -3.7171e-01, -1.1892e-01,  9.9600e-02,  3.8094e-02,\n",
       "          -5.1488e-02, -7.2162e-02, -2.3659e-01,  2.9590e-01, -4.1690e-01,\n",
       "           4.2417e-02,  4.2627e-01,  3.2469e-01, -5.8100e-01, -6.5207e-01,\n",
       "           6.4682e-01,  1.9149e-01, -3.1076e-02,  4.8234e-01,  3.0769e-02],\n",
       "         [ 1.9371e-01, -3.4400e-01, -9.9586e-02,  8.8870e-02,  2.6130e-02,\n",
       "          -5.9651e-02, -4.4329e-02, -2.0936e-01,  3.6876e-01, -3.9785e-01,\n",
       "           3.5745e-02,  4.4689e-01,  2.8836e-01, -5.2454e-01, -6.5672e-01,\n",
       "           6.2603e-01,  1.6124e-01, -1.1355e-02,  4.4079e-01,  1.9023e-02],\n",
       "         [ 2.2303e-01, -4.0057e-01, -1.1218e-01,  1.0711e-01,  4.9780e-02,\n",
       "          -5.2181e-02, -7.4624e-02, -2.5907e-01,  3.7891e-01, -4.3820e-01,\n",
       "           5.8924e-02,  5.0526e-01,  3.0253e-01, -6.2778e-01, -7.1974e-01,\n",
       "           7.1553e-01,  1.8913e-01, -4.4367e-03,  4.9548e-01,  3.5609e-02],\n",
       "         [ 1.4200e-01, -2.8111e-01, -7.0682e-02,  8.7481e-02,  5.8932e-02,\n",
       "          -4.5976e-02, -9.0191e-02, -1.7361e-01,  2.8860e-01, -3.2734e-01,\n",
       "           6.9660e-02,  4.1011e-01,  2.0626e-01, -5.0159e-01, -5.6120e-01,\n",
       "           5.4779e-01,  1.9970e-01, -4.8053e-03,  3.9070e-01,  6.2576e-03],\n",
       "         [ 1.5676e-01, -3.0319e-01, -9.2748e-02,  9.2563e-02,  4.8103e-02,\n",
       "          -6.4517e-02, -7.6546e-02, -1.8049e-01,  2.4900e-01, -3.5671e-01,\n",
       "           4.3700e-02,  3.5523e-01,  2.4083e-01, -5.0206e-01, -5.5458e-01,\n",
       "           5.3908e-01,  1.9300e-01, -2.3655e-02,  4.2364e-01,  1.7184e-02],\n",
       "         [ 1.8353e-01, -3.1157e-01, -6.2664e-02,  1.2639e-01,  3.5963e-02,\n",
       "          -5.6867e-02, -6.1181e-02, -1.7358e-01,  3.0642e-01, -3.0765e-01,\n",
       "           6.0445e-02,  3.6930e-01,  1.8318e-01, -4.7546e-01, -4.9644e-01,\n",
       "           5.2183e-01,  9.3585e-02, -3.0545e-02,  3.1531e-01,  2.1562e-02],\n",
       "         [ 2.2018e-01, -3.9919e-01, -1.1065e-01,  1.0768e-01,  5.2949e-02,\n",
       "          -5.0747e-02, -7.8250e-02, -2.5946e-01,  3.7461e-01, -4.3780e-01,\n",
       "           6.0223e-02,  5.0704e-01,  3.0148e-01, -6.2979e-01, -7.1908e-01,\n",
       "           7.1462e-01,  1.9393e-01, -2.5403e-03,  4.9734e-01,  3.5297e-02],\n",
       "         [ 1.9015e-01, -3.6944e-01, -1.1743e-01,  9.8730e-02,  3.8487e-02,\n",
       "          -5.0304e-02, -7.1469e-02, -2.3858e-01,  2.9656e-01, -4.1680e-01,\n",
       "           4.2342e-02,  4.2589e-01,  3.2271e-01, -5.8148e-01, -6.5086e-01,\n",
       "           6.4729e-01,  1.9314e-01, -3.0477e-02,  4.8331e-01,  3.2652e-02],\n",
       "         [ 1.7066e-01, -2.8034e-01, -4.9171e-02,  1.0614e-01,  6.4644e-02,\n",
       "          -4.2426e-02, -7.0809e-02, -2.0179e-01,  2.9374e-01, -3.4310e-01,\n",
       "           4.6676e-02,  4.0711e-01,  2.1634e-01, -4.9441e-01, -5.4730e-01,\n",
       "           5.4835e-01,  1.8838e-01, -2.8210e-03,  4.1270e-01,  2.2700e-02],\n",
       "         [ 1.8765e-01, -3.3755e-01, -1.0082e-01,  1.0568e-01,  2.2351e-02,\n",
       "          -8.2586e-02, -4.8510e-02, -1.5308e-01,  3.4655e-01, -3.5288e-01,\n",
       "           4.3733e-02,  4.0114e-01,  2.4666e-01, -4.7869e-01, -6.0035e-01,\n",
       "           5.6553e-01,  1.2062e-01, -2.6899e-02,  3.7657e-01, -1.0732e-02]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_head = AttentionHead(20, 20)\n",
    "attn_head(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "        assert d_model == d_feature * n_heads\n",
    "\n",
    "        self.attn_heads = nn.ModuleList([AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)])\n",
    "        self.projection = nn.Linear(d_feature * n_heads, d_model) \n",
    "    \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        \n",
    "        x = [attn(queries, keys, values, mask=mask) # (Batch, Seq, Feature)\n",
    "             for i, attn in enumerate(self.attn_heads)]\n",
    "        \n",
    "        # reconcatenate\n",
    "        x = torch.cat(x, dim=Dim.feature) # (Batch, Seq, D_Feature * n_heads)\n",
    "        x = self.projection(x) # (Batch, Seq, D_Model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0454,  0.0829, -0.0308,  ..., -0.0321,  0.1697, -0.0679],\n",
       "         [-0.0674,  0.1207, -0.0623,  ..., -0.0312,  0.1778, -0.0592],\n",
       "         [-0.0408,  0.1154, -0.0454,  ..., -0.0530,  0.1485, -0.0458],\n",
       "         ...,\n",
       "         [-0.0437,  0.1139, -0.0348,  ..., -0.0515,  0.1471, -0.0500],\n",
       "         [-0.0593,  0.1264, -0.0421,  ..., -0.0457,  0.1367, -0.0620],\n",
       "         [-0.0631,  0.1195, -0.0264,  ..., -0.0756,  0.1537, -0.0429]],\n",
       "\n",
       "        [[-0.0454,  0.0465, -0.0337,  ..., -0.0048,  0.1210, -0.0700],\n",
       "         [-0.0457,  0.0553, -0.0347,  ..., -0.0282,  0.1186, -0.0563],\n",
       "         [-0.0414,  0.0568, -0.0474,  ..., -0.0223,  0.1106, -0.0638],\n",
       "         ...,\n",
       "         [-0.0416,  0.0475, -0.0648,  ..., -0.0124,  0.1568, -0.0717],\n",
       "         [-0.0453,  0.0774, -0.0267,  ..., -0.0255,  0.1185, -0.0660],\n",
       "         [-0.0474,  0.0514, -0.0467,  ..., -0.0239,  0.1319, -0.0720]],\n",
       "\n",
       "        [[-0.0525,  0.1671,  0.0089,  ..., -0.0343,  0.1991, -0.0766],\n",
       "         [-0.0860,  0.1522, -0.0355,  ..., -0.0147,  0.2294, -0.0584],\n",
       "         [-0.0945,  0.1565, -0.0077,  ..., -0.0249,  0.2431, -0.0723],\n",
       "         ...,\n",
       "         [-0.0684,  0.1556, -0.0118,  ..., -0.0230,  0.2274, -0.0735],\n",
       "         [-0.0584,  0.1724, -0.0246,  ..., -0.0304,  0.1891, -0.0777],\n",
       "         [-0.0425,  0.1769,  0.0111,  ..., -0.0598,  0.2023, -0.0554]],\n",
       "\n",
       "        [[-0.0749,  0.1083, -0.0208,  ...,  0.0177,  0.1956, -0.1152],\n",
       "         [-0.0258,  0.1498, -0.0402,  ..., -0.0605,  0.1256, -0.0812],\n",
       "         [-0.0812,  0.0995, -0.0630,  ..., -0.0199,  0.1875, -0.1072],\n",
       "         ...,\n",
       "         [-0.0730,  0.1076, -0.0462,  ...,  0.0125,  0.1834, -0.1071],\n",
       "         [-0.0623,  0.1271, -0.0264,  ..., -0.0299,  0.1664, -0.0930],\n",
       "         [-0.0899,  0.1101, -0.0520,  ..., -0.0081,  0.1907, -0.1098]],\n",
       "\n",
       "        [[-0.0861,  0.1570, -0.0164,  ..., -0.0113,  0.1767, -0.0968],\n",
       "         [-0.0796,  0.1391, -0.0511,  ...,  0.0035,  0.1762, -0.1016],\n",
       "         [-0.0920,  0.1651, -0.0421,  ..., -0.0332,  0.1656, -0.1029],\n",
       "         ...,\n",
       "         [-0.0621,  0.1417, -0.0570,  ..., -0.0394,  0.1688, -0.1157],\n",
       "         [-0.1043,  0.1387, -0.0365,  ..., -0.0051,  0.2097, -0.1124],\n",
       "         [-0.0619,  0.1653, -0.0649,  ..., -0.0510,  0.1417, -0.0680]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = MultiHeadAttention(20 * 8, 20, 8)\n",
    "heads(q.repeat(1, 1, 8), \n",
    "      k.repeat(1, 1, 8), \n",
    "      v.repeat(1, 1, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2017-12-29-19.14.41.png?w=273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer normalization is similar to batch normalization, but normalizes across the feature dimension instead of the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-8):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder just stacks these together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    #level = TensorLoggingLevels.enc_dec_block\n",
    "    def __init__(self, d_model=512, d_feature=64, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(nn.Linear(d_model, d_ff),nn.ReLU(),nn.Linear(d_ff, d_model),)\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        att = self.attn_head(x, x, x, mask=mask)\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderBlock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7314, -0.9608,  1.9998,  ...,  0.2103, -1.5115,  1.8144],\n",
       "         [ 0.0362, -0.0593,  0.7419,  ...,  0.7844, -1.7380,  0.8663],\n",
       "         [-0.4143, -1.0543,  1.4343,  ..., -0.0920, -1.0956,  1.6937],\n",
       "         ...,\n",
       "         [-1.3945, -1.1517,  0.3591,  ...,  0.5138, -1.5281,  1.6051],\n",
       "         [ 0.0715, -0.3030,  3.0468,  ...,  0.4999, -1.3052, -0.5414],\n",
       "         [-0.7606, -0.2976,  1.5225,  ...,  0.5437, -0.6941, -1.0575]],\n",
       "\n",
       "        [[-0.2045, -2.0709,  1.8474,  ...,  1.0257, -1.2517,  0.2383],\n",
       "         [ 0.3157, -1.1443,  2.2165,  ...,  0.3258, -0.0598,  0.9028],\n",
       "         [ 0.1473, -2.2727,  0.7920,  ...,  0.3123, -0.6273,  0.7053],\n",
       "         ...,\n",
       "         [-1.1327, -1.7765,  1.9225,  ...,  0.7935,  0.1383,  2.2534],\n",
       "         [-0.7408, -1.2487,  1.1963,  ...,  0.7120, -1.0052,  1.2876],\n",
       "         [ 0.3173, -1.9165,  2.3517,  ...,  1.2001, -0.7337,  0.2320]],\n",
       "\n",
       "        [[-0.6179, -0.6417,  1.7838,  ...,  0.6559, -0.9354,  1.6094],\n",
       "         [ 0.4143, -0.4535,  1.4773,  ...,  1.0219, -1.2627,  0.9227],\n",
       "         [-1.0996, -1.2526,  1.5037,  ..., -0.0265, -1.0229, -0.1803],\n",
       "         ...,\n",
       "         [-0.6193, -0.8509,  1.1482,  ...,  0.0983, -0.1649,  0.0387],\n",
       "         [-0.1060, -0.2194,  1.6557,  ...,  2.0224, -1.2400,  0.2428],\n",
       "         [-0.0864, -1.3911,  0.9626,  ...,  0.6917, -0.9069,  1.3729]],\n",
       "\n",
       "        [[ 0.2891,  0.2394,  2.5150,  ...,  0.9630,  0.1677,  1.0787],\n",
       "         [-0.8702, -0.7591,  1.7506,  ...,  1.3834, -0.0927,  2.2941],\n",
       "         [-0.4079, -1.6328,  2.3710,  ...,  0.3242, -0.8336,  1.2114],\n",
       "         ...,\n",
       "         [-0.3039, -1.3132,  1.5215,  ...,  0.1810,  0.1866,  1.6063],\n",
       "         [ 0.2120, -1.9298,  2.4049,  ...,  0.4734, -0.5053,  1.8073],\n",
       "         [-0.0857, -2.0645,  0.9717,  ...,  1.1006, -1.2031,  1.8472]],\n",
       "\n",
       "        [[-0.1043, -0.3190,  1.6461,  ...,  0.4119, -1.7755,  0.4332],\n",
       "         [ 0.1079, -0.2225,  1.4695,  ...,  0.9162, -0.2261,  0.2356],\n",
       "         [ 0.0061, -0.1452,  2.4525,  ...,  0.5380, -0.6210,  1.7062],\n",
       "         ...,\n",
       "         [-1.3895, -1.7377,  2.8830,  ...,  0.7890, -0.2885,  0.8840],\n",
       "         [ 0.6704,  1.7042,  2.2040,  ...,  0.7860, -0.3464,  1.5263],\n",
       "         [ 0.6984, -1.9371,  2.8486,  ...,  1.8964, -0.7070,  0.8258]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(torch.rand(5, 10, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder consists of 6 consecutive encoder blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, n_blocks=6, d_model=512, n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([EncoderBlock(d_model=d_model, d_feature=d_model // n_heads,d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks)])\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor, mask=None):\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i1.wp.com/mlexplained.com/wp-content/uploads/2017/12/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2017-12-29-19.14.47.png?w=287)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys and values are the outputs of the encoder, and the queries are the outputs of the multi-head attention over the target entence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, d_feature=64, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(nn.Linear(d_model, d_ff),nn.ReLU(),nn.Linear(d_ff, d_model),)\n",
    "\n",
    "        self.layer_norm1 = LayerNorm(d_model)\n",
    "        self.layer_norm2 = LayerNorm(d_model)\n",
    "        self.layer_norm3 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_out, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        att = self.masked_attn_head(x, x, x, mask=src_mask)\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "\n",
    "        att = self.attn_head(queries=x, keys=enc_out, values=enc_out, mask=tgt_mask)\n",
    "        x = x + self.dropout(self.layer_norm2(att))\n",
    "\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.5143e-01,  4.0064e-01, -6.7979e-01,  ..., -3.1752e+00,\n",
       "          -2.3786e-01,  1.5174e+00],\n",
       "         [ 1.4182e+00,  4.7809e-01, -2.2517e-01,  ..., -1.5205e+00,\n",
       "          -1.4127e-01,  4.1145e-01],\n",
       "         [-5.8744e-01,  7.9096e-01,  2.1167e-01,  ..., -2.9919e+00,\n",
       "          -2.7412e-01,  3.7926e-01],\n",
       "         ...,\n",
       "         [-5.9609e-01,  1.1183e+00,  1.1979e+00,  ..., -3.5470e+00,\n",
       "           1.1903e-01, -6.7346e-01],\n",
       "         [-4.8036e-01,  7.7844e-01, -2.3372e+00,  ..., -2.4234e+00,\n",
       "          -7.4857e-01,  1.4273e+00],\n",
       "         [-2.8494e-01,  5.8207e-01, -1.1237e+00,  ..., -2.6507e+00,\n",
       "          -6.1364e-01,  7.5330e-01]],\n",
       "\n",
       "        [[-2.1470e-03,  1.4436e+00,  7.0646e-01,  ..., -1.6222e+00,\n",
       "          -8.7327e-02,  2.0979e+00],\n",
       "         [-1.0924e+00,  1.2902e+00, -5.1652e-01,  ..., -2.9347e+00,\n",
       "           2.8413e-01,  1.3538e+00],\n",
       "         [-6.2192e-01,  7.1885e-01,  1.2365e+00,  ..., -3.3762e+00,\n",
       "           9.6950e-01,  4.5544e-01],\n",
       "         ...,\n",
       "         [-6.4940e-01,  2.2826e+00, -1.9951e+00,  ..., -2.6225e+00,\n",
       "           6.6180e-01,  1.1068e+00],\n",
       "         [-7.1028e-01,  2.2568e-01, -3.4322e-01,  ..., -3.5229e+00,\n",
       "          -1.9222e-01,  3.9934e-01],\n",
       "         [-6.4381e-01,  1.1479e+00,  1.1096e+00,  ..., -2.2951e+00,\n",
       "          -1.3820e-02,  5.2121e-01]],\n",
       "\n",
       "        [[ 6.4133e-01,  4.8912e-01, -3.2758e-01,  ..., -2.6982e+00,\n",
       "          -2.9412e-01,  4.1169e-01],\n",
       "         [ 2.2454e-01, -1.9682e-01, -3.5789e-01,  ..., -1.1529e+00,\n",
       "          -4.1185e-01, -6.9912e-01],\n",
       "         [-1.3569e+00,  8.3414e-01,  8.4367e-01,  ..., -3.5415e+00,\n",
       "          -1.1761e+00,  5.1137e-01],\n",
       "         ...,\n",
       "         [ 2.2979e-01,  5.9183e-01, -2.2671e-01,  ..., -2.8755e+00,\n",
       "          -1.3404e-01,  2.5596e-01],\n",
       "         [ 7.5198e-01,  8.7918e-01, -4.2598e-01,  ..., -1.9163e+00,\n",
       "           2.8429e-02,  3.4192e-01],\n",
       "         [-3.3997e-01, -7.1775e-02,  1.0036e-01,  ..., -2.5420e+00,\n",
       "           9.1580e-01,  2.0755e+00]],\n",
       "\n",
       "        [[-5.0910e-01,  1.4348e+00,  4.1197e-01,  ..., -2.8284e+00,\n",
       "           6.9833e-01,  9.8034e-01],\n",
       "         [-1.4335e+00,  6.0524e-01,  7.2686e-01,  ..., -2.9756e+00,\n",
       "           1.0070e+00, -1.3756e-01],\n",
       "         [-1.2896e+00,  2.9279e-01,  1.0501e+00,  ..., -2.6179e+00,\n",
       "          -1.0152e+00,  9.3891e-01],\n",
       "         ...,\n",
       "         [-4.2636e-01,  1.3941e+00,  7.3188e-01,  ..., -3.0683e+00,\n",
       "           1.4891e+00,  6.4005e-02],\n",
       "         [-1.1142e+00,  1.8623e+00,  1.3757e-01,  ..., -2.4016e+00,\n",
       "           6.3614e-01, -2.2874e-01],\n",
       "         [ 6.7773e-01,  1.2273e+00,  1.5972e+00,  ..., -1.2311e-01,\n",
       "           8.6510e-01,  1.3456e+00]],\n",
       "\n",
       "        [[ 7.4446e-02,  1.0384e+00,  1.0995e+00,  ..., -6.0002e-01,\n",
       "           1.0469e+00,  1.0142e+00],\n",
       "         [ 1.0259e-01,  1.2264e+00,  1.1064e+00,  ..., -3.1762e+00,\n",
       "           1.3035e+00,  1.8555e+00],\n",
       "         [ 3.6826e-01,  1.2914e+00, -3.0260e-01,  ..., -2.0872e+00,\n",
       "          -4.7605e-01,  1.6718e+00],\n",
       "         ...,\n",
       "         [-5.4517e-01,  8.8919e-01, -3.1510e-01,  ...,  3.6625e-02,\n",
       "           3.2410e-01,  1.5179e-01],\n",
       "         [ 2.8760e-01,  8.7325e-01, -1.0007e+00,  ..., -3.0041e+00,\n",
       "          -1.6267e-01,  1.5912e+00],\n",
       "         [ 4.9563e-01,  1.1416e+00,  8.6800e-01,  ..., -2.1549e+00,\n",
       "           1.0267e+00,  6.5804e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = DecoderBlock()\n",
    "dec(torch.rand(5, 10, 512), enc(torch.rand(5, 10, 512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the decoder is just a stack of the underlying block so is simple to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, n_blocks=6, d_model=512, d_feature=64, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.decoders = nn.ModuleList([ DecoderBlock(d_model=d_model, d_feature=d_model // n_heads, d_ff=d_ff, dropout=dropout)\n",
    "            for _ in range(n_blocks) ])\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor, \n",
    "                enc_out: torch.FloatTensor, \n",
    "                src_mask=None, tgt_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, enc_out, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.weight = nn.Parameter(pe, requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.weight[:, :x.size(1), :] # (1, Seq, Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPositionEmbedding(nn.Module):\n",
    "    level = 1\n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.LongTensor, mask=None) -> torch.FloatTensor:\n",
    "        return self.word_embedding(x) + self.position_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.8228, -2.0909, -3.5735,  ...,  7.5374, -0.6125,  3.9951],\n",
       "         [ 1.9282, -3.8169,  2.0592,  ...,  4.8064,  1.6905,  1.4939],\n",
       "         [ 2.1767, -2.8520,  0.3860,  ...,  3.2723,  1.0018,  0.4706],\n",
       "         ...,\n",
       "         [ 2.2393, -0.1292,  3.8167,  ...,  9.2700,  2.1734,  6.1297],\n",
       "         [ 2.0363,  1.6934, -0.6265,  ...,  8.1327,  1.8512,  3.1298],\n",
       "         [ 2.8362, -2.5242,  1.1126,  ...,  5.5564, -2.2145,  3.3185]],\n",
       "\n",
       "        [[ 5.5173,  1.8708, -3.5587,  ...,  2.3345, -4.5050,  3.5049],\n",
       "         [ 6.4314, -0.6686, -0.2301,  ...,  4.5585, -1.9847,  3.4818],\n",
       "         [ 6.6054, -5.2696, -0.1708,  ...,  1.2646,  1.0728,  4.2250],\n",
       "         ...,\n",
       "         [ 5.5104, -1.6445, -2.6206,  ...,  5.0193, -4.9450,  2.0719],\n",
       "         [ 7.4420, -0.1016, -3.1009,  ...,  2.9020, -1.1202,  1.0917],\n",
       "         [ 1.3787, -2.1941, -0.5048,  ...,  2.3131,  0.9453,  6.2604]],\n",
       "\n",
       "        [[ 1.4098, -0.0258, -1.3463,  ...,  7.8112,  2.5633,  3.1469],\n",
       "         [ 6.4190, -0.4263, -1.5800,  ...,  6.4057, -2.2414,  7.1425],\n",
       "         [ 4.4018, -0.5251, -1.0229,  ...,  5.5738, -0.1998,  6.1713],\n",
       "         ...,\n",
       "         [ 2.8045,  0.8355, -0.7913,  ...,  7.5824, -4.8812,  8.0770],\n",
       "         [ 0.2471,  0.4974, -3.7503,  ...,  7.2658, -3.3048,  6.0256],\n",
       "         [ 4.1003,  2.6423,  0.5590,  ...,  8.5578, -1.5585,  7.4784]],\n",
       "\n",
       "        [[ 1.6262, -3.5521,  0.7061,  ...,  5.6746, -1.8625,  4.2511],\n",
       "         [ 8.5576, -1.6763, -0.2910,  ...,  4.0206, -0.0402,  3.8143],\n",
       "         [ 8.9002, -3.1994,  2.1630,  ...,  0.5712,  2.6848,  4.8155],\n",
       "         ...,\n",
       "         [ 5.2814, -6.9159,  4.6496,  ...,  2.6762,  0.6410,  3.9492],\n",
       "         [ 8.1513, -3.7337,  6.6473,  ...,  4.6692,  0.1391,  3.4618],\n",
       "         [ 0.9434, -6.4580,  0.1548,  ...,  4.1694,  1.4835,  3.5753]],\n",
       "\n",
       "        [[ 5.3026,  2.7134, -2.5735,  ..., -0.5854,  2.0258,  4.0264],\n",
       "         [ 1.3348,  1.2019,  0.1268,  ...,  4.1940, -2.5756,  4.1606],\n",
       "         [ 7.9750, -1.8662, -0.6754,  ...,  3.2190,  2.0551,  1.9537],\n",
       "         ...,\n",
       "         [ 3.1900, -0.6341,  1.2282,  ..., -1.3123, -0.2805,  5.3124],\n",
       "         [ 2.6356, -0.5034,  2.4822,  ...,  1.8872,  0.4330,  2.7207],\n",
       "         [ 3.3294, -2.7320, -1.5482,  ...,  5.2499,  0.5786,  4.9255]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(emb(torch.randint(1000, (5, 30))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://camo.githubusercontent.com/88e8f36ce61dedfd2491885b8df2f68c4d1f92f5/687474703a2f2f696d6775722e636f6d2f316b72463252362e706e67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = WordPositionEmbedding(1000)\n",
    "encoder = TransformerEncoder()\n",
    "decoder = TransformerDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.5089, -4.8524,  8.9166,  ...,  4.8481,  3.4639, -3.0672],\n",
       "         [-4.4578,  0.6768,  7.1931,  ...,  7.4993,  2.0882, -2.2992],\n",
       "         [-4.3852, -1.9266,  8.4774,  ...,  7.7719, -0.1840, -6.2831],\n",
       "         ...,\n",
       "         [-7.4598, -4.0433,  3.9511,  ...,  7.1757,  0.6233, -3.7613],\n",
       "         [-5.0434, -3.1148,  3.9955,  ...,  7.7747, -2.7859, -3.6723],\n",
       "         [-5.1826,  0.1430,  6.7114,  ...,  6.5203,  0.2865, -4.3229]],\n",
       "\n",
       "        [[-3.7279,  1.9699,  2.8120,  ...,  6.3024,  1.5573, -1.8119],\n",
       "         [-2.8885, -1.1445,  4.2325,  ..., 10.5152, -1.4743, -3.4741],\n",
       "         [-2.1535,  3.6287,  2.9028,  ...,  2.1750,  3.2068, -4.6496],\n",
       "         ...,\n",
       "         [-4.0922,  2.3624,  6.2289,  ...,  8.4165,  2.6457,  1.1291],\n",
       "         [-2.2871,  1.4994,  5.5360,  ...,  8.1237, -1.0908, -5.8973],\n",
       "         [-8.6225,  4.3793,  7.1774,  ...,  7.8179,  2.8150, -3.5142]],\n",
       "\n",
       "        [[-6.7801,  2.1838,  5.9175,  ...,  8.5489,  7.5672, -4.0281],\n",
       "         [-6.9171,  0.5173,  4.3053,  ...,  6.4676,  4.2396, -0.5467],\n",
       "         [-4.8369, -2.8228,  3.3916,  ..., 12.1051,  6.8612, -2.1810],\n",
       "         ...,\n",
       "         [-1.9900,  0.7556,  4.9973,  ..., 10.2569,  4.6742, -2.3227],\n",
       "         [-4.9509, -1.2160,  3.3238,  ..., 10.1379,  7.2027,  1.8330],\n",
       "         [-5.0667, -0.7891,  2.6456,  ...,  4.6747,  2.5576, -2.1217]],\n",
       "\n",
       "        [[-2.6952,  0.8140,  2.7419,  ...,  3.3617, -0.1698,  2.4003],\n",
       "         [-4.0180,  2.5784, -2.1250,  ...,  7.4670,  2.5404, -4.7081],\n",
       "         [-2.3342,  0.9853,  6.6578,  ...,  8.0362,  0.5110, -1.6931],\n",
       "         ...,\n",
       "         [-6.2040,  6.0095,  5.5549,  ...,  7.2113,  1.0284, -4.3341],\n",
       "         [-3.4077, -0.1126,  4.4126,  ...,  4.2492, -2.3519, -5.8186],\n",
       "         [-7.1347,  3.0987,  5.7057,  ...,  7.5342, -0.9517, -4.3195]],\n",
       "\n",
       "        [[-4.0552,  0.5516,  4.5649,  ..., 11.6660,  2.3085, -2.7501],\n",
       "         [-4.1751,  0.4200,  4.4967,  ..., 10.2290,  0.5702, -6.1563],\n",
       "         [-5.0107,  1.7350,  8.9853,  ...,  9.1698,  4.5443, -1.5097],\n",
       "         ...,\n",
       "         [-4.8922, -0.5252,  6.9249,  ...,  5.9207,  1.4888, -1.9593],\n",
       "         [-2.1401,  2.6277,  2.6523,  ...,  9.2994,  4.1349, -3.8377],\n",
       "         [-3.3589,  0.9325,  2.8843,  ...,  9.1443,  3.6206, -3.3440]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_ids = torch.randint(1000, (5, 30))\n",
    "tgt_ids = torch.randint(1000, (5, 30))\n",
    "x = encoder(emb(src_ids))\n",
    "decoder(emb(tgt_ids), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
